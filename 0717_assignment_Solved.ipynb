{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment on Regularization and Optimization of Deep Learning\n",
    "\n",
    "이번 과제에서는 reagularization과 optimization에서 배운 내용들을 이용해 최대한 Deep Models의 성능을 높여보고자 합니다. Layer 4개짜리 MLP (각 hidden layer는 512개의 unit을 가짐) 상황에서 정규화와 최적화 방법론들을 총 동원해 성능을 높여주시면 됩니다.\n",
    "\n",
    "먼저, 아래 코드는 데이터 셋을 셋팅하는 부분입니다. 이 부분은 건드리시면 안됩니다. 이 부분을 건드리시면 0점 처리 됩니다. 외부 데이터 사용하셔도 안됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 32, 32, 3)\n",
      "(1000,)\n",
      "(5000, 32, 32, 3)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import random \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed=seed)\n",
    "tf.random.set_random_seed(seed)\n",
    "\n",
    "(x_1, y_1), (x_2, y_2) = tf.keras.datasets.cifar100.load_data()\n",
    "x_total = np.concatenate([x_1, x_2], axis=0).astype(np.float64)\n",
    "y_total = np.concatenate([y_1, y_2], axis=0)\n",
    "\n",
    "n_output = 10\n",
    "\n",
    "valid_index, _ = np.where(y_total < n_output)\n",
    "y_total = y_total[valid_index].reshape([-1])\n",
    "x_total = x_total[valid_index]\n",
    "\n",
    "i = np.arange(x_total.shape[0])\n",
    "np.random.shuffle(i)\n",
    "x_total = x_total[i]\n",
    "y_total = y_total[i]\n",
    "\n",
    "train_size = 100 * n_output\n",
    "x_train = x_total[:train_size]\n",
    "y_train = y_total[:train_size]\n",
    "x_test = x_total[train_size:]\n",
    "y_test = y_total[train_size:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation set을 나눕니다. \n",
    "- 실습시간에 배웠던 것처럼 Validation set 비율은 조정하셔도 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = x_train.shape[0] // 5\n",
    "x_valid = x_train[:split]\n",
    "y_valid = y_train[:split]\n",
    "\n",
    "x_train = x_train[split:]\n",
    "y_train = y_train[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지를 greyscale로 변경합니다. \n",
    "1. RGB 값을 고려한 코드로 변경하셔도 됩니다. \n",
    "2. Augmentation을 고려해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 32, 32)\n",
      "(200, 32, 32)\n",
      "(5000, 32, 32)\n",
      "(800, 1024)\n",
      "(200, 1024)\n",
      "(5000, 1024)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.mean(x_train, axis=3)\n",
    "x_valid = np.mean(x_valid, axis=3)\n",
    "x_test = np.mean(x_test, axis=3)\n",
    "print(x_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "n_input = 32 * 32\n",
    "\n",
    "x_train = x_train.reshape([-1, n_input])\n",
    "x_valid = x_valid.reshape([-1, n_input])\n",
    "x_test = x_test.reshape([-1, n_input])\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델을 만듭니다.\n",
    "\n",
    "1. Optimizer를 다른 걸로 바꿔보세요\n",
    "2. Learning Rate를 바꿔보세요. Learning Rate Scheduling도 고려해보세요.\n",
    "3. Activation Function을 바꿔보세요. \n",
    "4. Dropout, DropConnect, Gaussian Dropout 을 고려해보세요.\n",
    "5. Augmentation을 고려해보세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "n_units = [n_input, 512, 512, 512, n_output]\n",
    "\n",
    "weights, biases = [], []\n",
    "for i, (n_in, n_out) in enumerate(zip(n_units[:-1], n_units[1:])):\n",
    "    stddev = math.sqrt(2 / n_in) # Kaiming He Initialization\n",
    "    weight = tf.Variable(tf.random.truncated_normal([n_in, n_out], mean=0, stddev=stddev))\n",
    "    bias = tf.Variable(tf.zeros([n_out]))\n",
    "    weights.append(weight)\n",
    "    biases.append(bias)    \n",
    "    \n",
    "layer = x \n",
    "\n",
    "for i, (weight, bias) in enumerate(zip(weights, biases)):\n",
    "    layer = tf.matmul(layer, weight) + bias\n",
    "    if i < len(weights) - 1:\n",
    "        \n",
    "        layer = tf.nn.tanh(layer) \n",
    "        # layer = tf.nn.sigmoid(layer)\n",
    "        # layer = tf.nn.relu(layer)\n",
    "        \n",
    "        layer = tf.keras.layers.GaussianDropout(rate=0.3)(layer, training=training)\n",
    "        # layer = tf.nn.dropout(layer, keep_prob=0.5)*0.5   # Drop connect\n",
    "        # layer = tf.layers.dropout(layer, rate=0.4, training=training)  # Drop out (layers Dropout)    \n",
    "        \n",
    "        \n",
    "y_hat = layer\n",
    "\n",
    "y_hot = tf.one_hot(y, n_output)\n",
    "costs = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        labels=y_hot, logits=y_hat)\n",
    "cross_entropy_loss = tf.reduce_mean(costs)\n",
    "loss = cross_entropy_loss \n",
    "\n",
    "y_label = tf.argmax(y_hat, 1)\n",
    "accuracy = tf.count_nonzero(\n",
    "        tf.cast(tf.equal(tf.argmax(y_hot, 1), y_label),\n",
    "                tf.int64)) / tf.cast(tf.shape(y_hot)[0], tf.int64)\n",
    "\n",
    "extra_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "\n",
    "with tf.control_dependencies(extra_ops):     \n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(1e-4)    \n",
    "    # optimizer = tf.train.AdamOptimizer(1e-3)    \n",
    "    # optimizer = tf.train.AdamOptimizer(1e-5)    \n",
    "    # optimizer = tf.train.GradientDescentOptimizer(1e-4)\n",
    "    # optimizer = tf.train.RMSPropOptimizer(1e-4)\n",
    "    # optimizer = tf.train.AdagradOptimizer(1e-4)    \n",
    " \n",
    "    train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 2.1106 2.3066 2.2474 0.2338 0.1350 0.1698\n",
      "20 1.9816 2.1953 2.1651 0.3050 0.1500 0.2084\n",
      "30 1.8698 2.1782 2.1287 0.3475 0.2150 0.2410\n",
      "40 1.7896 2.1558 2.1038 0.3925 0.2150 0.2610\n",
      "50 1.7074 2.1338 2.0967 0.4200 0.2350 0.2720\n",
      "60 1.6562 2.1240 2.0787 0.4400 0.2550 0.2816\n",
      "70 1.5941 2.1205 2.0874 0.4525 0.2850 0.2870\n",
      "80 1.5394 2.1116 2.0872 0.4738 0.2800 0.2876\n",
      "90 1.4983 2.1618 2.0953 0.4888 0.2450 0.2940\n",
      "100 1.4582 2.1381 2.1044 0.5000 0.2550 0.2882\n",
      "110 1.4188 2.1496 2.0899 0.5162 0.2650 0.2960\n",
      "120 1.3730 2.1803 2.1298 0.5312 0.2400 0.2960\n",
      "130 1.3132 2.1913 2.1145 0.5613 0.2650 0.2958\n",
      "140 1.2787 2.1318 2.1289 0.5813 0.2900 0.3020\n",
      "150 1.2450 2.1681 2.1439 0.5713 0.2600 0.3020\n",
      "160 1.1820 2.1696 2.1505 0.6212 0.2800 0.3022\n",
      "170 1.1206 2.1877 2.1614 0.6575 0.2800 0.3028\n",
      "180 1.0917 2.2416 2.1935 0.6462 0.3050 0.3072\n",
      "190 1.0423 2.2084 2.2058 0.6462 0.2600 0.3096\n",
      "200 0.9848 2.2550 2.2200 0.6837 0.2700 0.3042\n",
      "210 0.9666 2.2712 2.2307 0.7025 0.2650 0.3054\n",
      "220 0.9398 2.2880 2.2680 0.7087 0.2700 0.3074\n",
      "230 0.8919 2.2990 2.2623 0.7175 0.2900 0.3124\n",
      "240 0.8658 2.3211 2.2786 0.7163 0.2800 0.3102\n",
      "250 0.8044 2.3153 2.3111 0.7538 0.2950 0.3086\n",
      "260 0.7642 2.3506 2.3236 0.7600 0.2750 0.3074\n",
      "270 0.7518 2.3390 2.3603 0.7688 0.3000 0.3062\n",
      "280 0.7031 2.3731 2.3942 0.7850 0.2750 0.3050\n",
      "0.3072\n"
     ]
    }
   ],
   "source": [
    "gpu_options = tf.GPUOptions()\n",
    "gpu_options.allow_growth = True\n",
    "session = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "max_valid_epoch_idx = 0\n",
    "max_valid_accuracy = 0.0\n",
    "final_test_accuracy = 0.0\n",
    "for epoch_idx in range(1, 10000 + 1):    \n",
    "    \n",
    "    \n",
    "    session.run(\n",
    "            train_op,\n",
    "            feed_dict={\n",
    "                x: x_train,\n",
    "                y: y_train,\n",
    "                training: True\n",
    "            })\n",
    "    \n",
    "    if epoch_idx % 10 == 0:\n",
    "        train_loss_value, train_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_train,\n",
    "                y: y_train,\n",
    "                training: False\n",
    "            })\n",
    "        \n",
    "        valid_loss_value, valid_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_valid,\n",
    "                y: y_valid,\n",
    "                training: False\n",
    "            })\n",
    "            \n",
    "        test_loss_value, test_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_test,\n",
    "                y: y_test,\n",
    "                training: False\n",
    "            })\n",
    "\n",
    "        print(epoch_idx, '%.4f' % train_loss_value, '%.4f' % valid_loss_value, '%.4f' % test_loss_value, '%.4f' % train_accuracy_value, '%.4f' % valid_accuracy_value, '%.4f' % test_accuracy_value)\n",
    "        \n",
    "        if max_valid_accuracy < valid_accuracy_value:\n",
    "            max_valid_accuracy = valid_accuracy_value \n",
    "            max_valid_epoch_idx = epoch_idx\n",
    "            final_test_accuracy = test_accuracy_value\n",
    "            \n",
    "    \n",
    "    # Early Stop\n",
    "    if max_valid_epoch_idx + 100 < epoch_idx:\n",
    "        break\n",
    "        \n",
    "print(final_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32.88% 의 성능을 확인할 수 있습니다. 실습시간 배운 몇 가지 정규화와 최적화 과정을 동원하면 50% 정도의 성능까지는 쉽게 달성할 수 있음을 확인했습니다. 수업시간에 배운 내용들을 사용해 최대한 높은 성능을 나타내는 모델을 만들어보세요! \n",
    "주피터 노트북 파일을 제출해주시면 되며, 성능을 기준으로 점수를 매길 예정입니다. (상대평가)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
